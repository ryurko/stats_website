<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Workshops on Ron Yurko</title>
    <link>http://www.stat.cmu.edu/~ryurko/post/</link>
    <description>Recent content in Workshops on Ron Yurko</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Ron Yurko</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/~ryurko/post/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Bayesian Baby Steps: Normal Next Steps</title>
      <link>http://www.stat.cmu.edu/~ryurko/post/bayesian-baby-steps-normal-next-steps/</link>
      <pubDate>Wed, 04 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.stat.cmu.edu/~ryurko/post/bayesian-baby-steps-normal-next-steps/</guid>
      <description>&lt;div id=&#34;enter-marquis-de-laplace&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Enter marquis de Laplace&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://www.stat.cmu.edu/~ryurko/post/bayesian-baby-steps-intro/&#34;&gt;In my first post on Bayesian data analysis,&lt;/a&gt;
I did a brief overview of how Bayesian updating works using grid approximation
to arrive at posterior distributions for our parameters of interest, such as a
wide receiver’s catch rate. While the grid-based approach is simple and easy to
follow, it’s just not practical. Before we turn to MCMC, in this post we’ll
cover the popular approach known as Laplace approximation&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;, aka quadratic approximation.&lt;/p&gt;
&lt;p&gt;As I said before, in many cases we’re not able to derive the posterior
distribution so some sort of approximation method is required. The one and only
&lt;a href=&#34;https://en.wikipedia.org/wiki/Pierre-Simon_Laplace&#34;&gt;Pierre-Simon Laplace&lt;/a&gt;&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;
had the realization you can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Normal_distribution&#34;&gt;Gaussian (normal) distribution (good old bell curve)&lt;/a&gt; for approximating the
posterior if its roughly symmetric and unimodal. One of the great benefits of
working with the Gaussian distribution is that you only need two numbers to
describe it: the mean &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; for the center, and the variance &lt;span class=&#34;math inline&#34;&gt;\(\sigma^2\)&lt;/span&gt; for the
spread.&lt;/p&gt;
&lt;p&gt;The reason it’s often called quadratic approximation is because we use a
&lt;em&gt;quadratic&lt;/em&gt; function for approximating the logarithm of the posterior density,
which we’ll denote as &lt;span class=&#34;math inline&#34;&gt;\(p(\theta |x)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(x\)&lt;/span&gt; is our observed data and &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt;
represents our parameter(s) of interest. This approximation uses the fact that
the log of a Gaussian is a parabola - a quadratic function. You simply take a
&lt;a href=&#34;https://en.wikipedia.org/wiki/Taylor_series#Definition&#34;&gt;Taylor series expansion&lt;/a&gt;
of the log of the posterior density centered at the posterior mode &lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}\)&lt;/span&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{log}\  p(\theta|x) = \text{log}\  p(\hat{\theta}|x) + \frac{1}{2}(\theta - \hat{\theta})^T \Big[ \frac{d^2}{d\theta^2} \text{log} p(\theta | x) \Big]_{\theta = \hat{\theta}}(\theta - \hat{\theta}) +\  ... 
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let me break this down. The first thing you might be thinking is, “Wait how do we
find the posterior mode???” - well it’s actually pretty simple with a standard
optimization procedure (as you’ll see below). This mode represents the center of
the Gaussian that we’ll use to approximate the posterior. The posterior mode is also
referred to by its Latin name&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; &lt;em&gt;maximum a-posteriori&lt;/em&gt;, denoted by
&lt;span class=&#34;math inline&#34;&gt;\(\hat{\theta}_{MAP}\)&lt;/span&gt;. The first term in the expansion above is merely a constant,
but the second term provides us an estimate of the curvature near the posterior’s
peak and is proportional to the log of a Gaussian density. This provides us with
the variance of the Gaussian to approximate the posterior as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p(\theta|x) \approx \text{N}\big( \hat{\theta}, [I(\hat{\theta})]^{-1} \big)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(I(\hat{\theta})\)&lt;/span&gt; is an estimate for &lt;span class=&#34;math inline&#34;&gt;\(I(\theta)\)&lt;/span&gt;, the &lt;em&gt;observed information&lt;/em&gt;,&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
I(\theta) = -\frac{d^2}{d\theta^2} \text{log}\  p(\theta|x).
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That’s all we need for this approximation, the posterior mode and the curvature
of the posterior density and BAM we have the entire posterior distribution. Well
really an approximation of it - but from asymptotic theory the posterior
distribution approaches a Gaussian under some conditions. I won’t get into the
details, so definitely &lt;a href=&#34;https://en.wikipedia.org/wiki/Laplace%27s_method&#34;&gt;check out more of the theory&lt;/a&gt;
yourself. I also recommend chapter four of the classic &lt;a href=&#34;https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954&#34;&gt;Bayesian Data Analysis&lt;/a&gt;
by Gelman et al.&lt;/p&gt;
&lt;p&gt;The excellent blog by Rasmus Bååth also has a &lt;a href=&#34;http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/&#34;&gt;post on Laplace approximation&lt;/a&gt;
with a binomial example, like the catch rate model in the previous post, showing
how the approximation using a Gaussian improves with more data despite the fact
the posterior is actually a beta! I’ll use a different example for the rest of
this post and demonstrate the usage of quadratic approximation for Bayesian
linear regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modeling-nfl-score-differential&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modeling NFL score differential&lt;/h2&gt;
&lt;p&gt;Keeping with my theme of working with American football data&lt;a href=&#34;#fn4&#34; class=&#34;footnote-ref&#34; id=&#34;fnref4&#34;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;, I’ll demonstrate
the use of quadratic approximation with a model for an individual team’s
score differential. The first step is to gather the score differential for each
team in each season, which can be done using the &lt;a href=&#34;https://github.com/maksimhorowitz/nflscrapR&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;&lt;/a&gt;
package. An easier route however is to access the data from my &lt;a href=&#34;https://ryurko.github.io/nflscrapR-data/&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;-data repository&lt;/a&gt;. Rather than scavenging
through those files, in my &lt;a href=&#34;https://github.com/ryurko/nflWAR&#34;&gt;&lt;code&gt;nflWAR&lt;/code&gt; package&lt;/a&gt;
there’s a function called &lt;code&gt;get_season_summary()&lt;/code&gt; that takes in a vector of years
and returns a data frame with rows for each team’s individual season, and has columns
for their number of wins and total score differential. The code chunk below
demonstrates how to use gather this data for NFL seasons from 2009 to 2017:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load the tidyverse
# install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)

# Use the nflWAR package to get summaries of team performances for each season,
# first install devtools if you don&amp;#39;t have then install nflWAR:
# install.packages(&amp;quot;devtools&amp;quot;)
# devtools::install_github(&amp;quot;ryurko/nflWAR&amp;quot;)
library(nflWAR)

# Use the get_season_summary() function to create a dataset that has a row
# for team-season with a column for their score differential from 2009 to 2017:
team_summary_df &amp;lt;- get_season_summary(2009:2017)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before going into the regression example with a predictor, it’s worthwhile to
first demonstrate quadratic approximation by just modeling the score differential
with a Gaussian. First step is to visualize the score differential distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Visualize the distribution with the points colored by year:
ggplot(team_summary_df, aes(x = Total_Score_Diff)) +
  geom_histogram(aes(y = ..density..), bins = 20) + theme_bw() + 
  scale_x_continuous(limits = c(-300, 300)) +
  labs(x = &amp;quot;Score differential&amp;quot;, y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Distribution of individual team score differential in each season from 2009-17 &amp;quot;,
       caption = &amp;quot;Data accessed with nflscrapR&amp;quot;) +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this histogram we have support for using a Gaussian, since it’s roughly
symmetric and unimodal. Also, since an observed &lt;em&gt;season&lt;/em&gt; score differential is the
sum of the score differentials in &lt;em&gt;each game&lt;/em&gt;, we can think of these values as
following a random walk. Some teams will have large positive differentials, which
means some team will have large negative differentials. I won’t go into the details
but theoretically this process of summing together random values, assumed to be
generated from the same distribution, &lt;a href=&#34;https://arxiv.org/pdf/0906.3507.pdf&#34;&gt;will follow a Gaussian distribution&lt;/a&gt;.
In other words - the Gaussian distribution is a reasonable choice here.&lt;/p&gt;
&lt;p&gt;We assume then that the general model for an observed score differential &lt;span class=&#34;math inline&#34;&gt;\(S_i\)&lt;/span&gt;,
where &lt;span class=&#34;math inline&#34;&gt;\(i = 1,...,\)&lt;/span&gt; 288 is the index for an observed
team-season score differential, follows a Gaussian distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_i \sim \text{N}(\mu, \sigma^2)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Because we’re approaching this problem from a Bayesian perspective, we really
want to consider an infinite number of possible Guassians based on all of the
possible combinations for our parameters &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. Of course we’re
not going to actually look at every possible value for these parameters,
laplace approximation will accomplish what we need. But, as I pointed out in the
previous post, we’re interested in the entire posterior distribution - which in this case
is a posterior distribution &lt;em&gt;of distributions&lt;/em&gt;. So for a full model of score
differential, we need priors for both &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_i \sim \text{N}(\mu, \sigma^2) \\
\mu \sim \text{N}(0, 100^2) \\
\sigma \sim \text{Uniform}(0,200)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This is a really naive model, just to demonstrate Laplace approximation. Next
we’ll implement the same code in the &lt;a href=&#34;http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/&#34;&gt;Bååth blog post&lt;/a&gt;
which uses the &lt;code&gt;optim&lt;/code&gt; function in &lt;code&gt;R&lt;/code&gt; to find the mode of the posterior and
also returns the &lt;a href=&#34;https://en.wikipedia.org/wiki/Hessian_matrix&#34;&gt;Hessian matrix&lt;/a&gt;
for the curvature (following the observed information definition above.
For ease, we’ll roughly follow his example code which also uses
the &lt;a href=&#34;https://cran.r-project.org/web/packages/mvtnorm/index.html&#34;&gt;&lt;code&gt;mvtnorm&lt;/code&gt;&lt;/a&gt;
packages for sampling from the approximated posterior (this sampling isn’t
really necessary but it’s good practice to start sampling from the posterior).
We’re working with two parameters here, so the resulting posterior approximation
is a multivariate Gaussian, hence why we use the &lt;code&gt;rmvnorm()&lt;/code&gt; function since we’re
not only working with each parameter’s mean and variance, but their covariance
as well (in this example the covariance is essentially 0 but that
won’t typically be the case).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;mvtnorm&amp;quot;)

#&amp;#39; Function to perform simple Laplace approximation
#&amp;#39; @param log_posterior_n Function that returns the log posterior given a vector
#&amp;#39; of parameter values and observed data.
#&amp;#39; @param init_points Vector of starting values for parameters.
#&amp;#39; @param n_samples Number of samples to draw from resulting approximated 
#&amp;#39; posterior distribution for then visualizing.
#&amp;#39; @param ... Any additional arguments passed to the log_posterior_fun during 
#&amp;#39; the optimization.
#&amp;#39; @return Samples from approximated posterior distribution.
laplace_approx &amp;lt;- function(log_posterior_fn, init_points, n_samples, ...) {
  
  # Use the optim function with the input starting points and function to
  # evaluate the log posterior to find the mode and curvature with hessian = TRUE
  # (note that using fnscale = -1 means to maximize). We use the ... here to 
  # allow for flexible name of the data in log_posterior_fn:
  fit &amp;lt;- optim(init_points, log_posterior_fn, 
               # Use the same quasi-Newton optimization method as McElreath&amp;#39;s
               # map function in his rethinking package:
               method = &amp;quot;BFGS&amp;quot;,
               # using fnscale = -1 means to maximize
               control = list(fnscale = -1), 
               # need the hessian for the curvature
               hessian = TRUE, 
               # additional arguments for the function we&amp;#39;re optimizing
               ...)
  
  # Store the mean values for the parameters and curvature to then generate
  # samples from the approximated normal posterior given the number of samples
  # for both of the parameters, returning as a data frame:
  param_mean &amp;lt;- fit$par
  # inverse of the negative hessian for the covariance - same as the use 
  # of the observed information in the intro
  param_cov_mat &amp;lt;- solve(-fit$hessian)
  # sample from the resulting joint posterior to get posterior distributions
  # for each parameter:
  mvtnorm::rmvnorm(n_samples, param_mean, param_cov_mat) %&amp;gt;%
    data.frame()
}

#&amp;#39; Function to calculate log posterior for simple score differential model
#&amp;#39; @param params Vector of parameter values that are named &amp;quot;mu&amp;quot; and &amp;quot;sigma&amp;quot;.
#&amp;#39; @param score_diff_values Vector of observed score differential values
score_diff_model &amp;lt;- function(params, score_diff_values) {
  
  # Log likelihood:
  sum(dnorm(score_diff_values, params[&amp;quot;mu&amp;quot;], params[&amp;quot;sigma&amp;quot;], log = TRUE)) +
    # plus the log priors results in log posterior:
    dnorm(params[&amp;quot;mu&amp;quot;], 0, 100, log = TRUE) + 
    dunif(params[&amp;quot;sigma&amp;quot;], 0, 200, log = TRUE)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With these functions we’ll now approximate the posterior, considering very naive
starting values for our optimization with &lt;span class=&#34;math inline&#34;&gt;\(\mu = 200\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\sigma = 10\)&lt;/span&gt; (this is
just to demonstrate, I have no reason for choosing these values).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init_samples &amp;lt;- laplace_approx(log_posterior_fn = score_diff_model,
                               init_points = c(mu = 200, sigma = 10),
                               n_samples = 10000,
                               # Specify the data for the optimization!
                               score_diff_values = team_summary_df$Total_Score_Diff)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can easily view the joint distribution of our posterior samples with their
respective marginals on the side (&lt;a href=&#34;https://twitter.com/ClausWilke/status/900776341494276096/photo/1?ref_src=twsrc%5Etfw&amp;amp;ref_url=http%3A%2F%2Flreding.com%2Fnonstandard_deviations%2F2017%2F08%2F19%2Fcowmarg%2F&#34;&gt;code courtesy of Claus Wilke using &lt;code&gt;cowplot&lt;/code&gt;&lt;/a&gt;):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;cowplot&amp;quot;)
library(cowplot)

# First the joint distribution plot with points for the values:
joint_plot &amp;lt;- ggplot(init_samples, aes(x = mu, y = sigma)) + 
  geom_point(alpha = .3, color = &amp;quot;darkblue&amp;quot;)  +
  labs(title = &amp;quot;Posterior distribution for model parameters with marginals&amp;quot;) +
  theme_bw() +
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 16))

# Marginal density for mu along x-axis using the cowplot function axis_canvas:
mu_dens &amp;lt;- axis_canvas(joint_plot, axis = &amp;quot;x&amp;quot;) +
  geom_density(data = init_samples, aes(x = mu), fill = &amp;quot;darkblue&amp;quot;,
               alpha = 0.5, size = .2)

# Same thing for sigma but along y and with coord_flip = TRUE to make it vertical:
sigma_dens &amp;lt;- axis_canvas(joint_plot, axis = &amp;quot;y&amp;quot;, coord_flip = TRUE) +
  geom_density(data = init_samples, aes(x = sigma), fill = &amp;quot;darkblue&amp;quot;,
               alpha = 0.5, size = .2) +
  coord_flip()

# Now generate by adding these objects to the main plot:
# Need grid:
# install.packages(&amp;quot;grid&amp;quot;)
joint_plot_1 &amp;lt;- insert_xaxis_grob(joint_plot, mu_dens, 
                                  grid::unit(.2, &amp;quot;null&amp;quot;), position = &amp;quot;top&amp;quot;)
joint_plot_2 &amp;lt;- insert_yaxis_grob(joint_plot_1, sigma_dens, 
                                  grid::unit(.2, &amp;quot;null&amp;quot;), position = &amp;quot;right&amp;quot;)
ggdraw(joint_plot_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this we can clearly see the Gaussian approximations for both of our
parameters with the distribution of &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; centered around 0 and the distribution
for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; centered around 100. Remember this represents the distributions
for the parameters of our score differential model. So sampling from this posterior
means we are generating different Gaussian distributions for the score differential.
My next post will focus on sampling from the posterior, but to give you a taste
of what I mean the code below uses these 10000 values from &lt;code&gt;init_samples&lt;/code&gt; for
each parameter, and then samples 10000 values from distributions using these
combinations of values to give us our approximate score differential distribution.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# R is vectorized so can just give it the vector of values for the Gaussian
# distribution parameters to create the simulated score-diff distribution:
sim_score_diff_data &amp;lt;- data.frame(Total_Score_Diff = 
                                    rnorm(10000, mean = init_samples$mu,
                                          sd = init_samples$sigma))

# Now generate the histogram from before but now add this simulated density
# on top:
ggplot(team_summary_df, aes(x = Total_Score_Diff)) +
  # Specifying y = ..density.. uses density instead of counts
  geom_histogram(aes(y = ..density..), bins = 20) + 
  # Add the density curve
  geom_density(data = sim_score_diff_data, 
               aes(x = Total_Score_Diff), fill = NA, color = &amp;quot;darkblue&amp;quot;) +
  scale_x_continuous(limits = c(-300, 300)) +
  theme_bw() + 
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14)) +
  labs(x = &amp;quot;Score differential&amp;quot;, y = &amp;quot;Count&amp;quot;,
       title = &amp;quot;Distribution of individual team score differential in each season from 2009-17 &amp;quot;,
       caption = &amp;quot;Data accessed with nflscrapR&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Not bad, but then again this was a pretty easy example. You could also
use the grid approximation from the last post to accomplish the same task but
it starts to be annoying to work with even with just two parameters, quickly
becoming unbearable with more and more parameters.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;how-to-win-in-the-nfl&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;How to win in the NFL&lt;/h2&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media2.giphy.com/media/FESGhqDEWHAL6/giphy-downsized.gif?cid=e1bb72ff5b3baa416a2f74554d78d0fe&#34; alt=&#34;Step 1: avoid this man&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Step 1: avoid this man&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;Ok, so we’ve seen how easy it is to use Laplace approximation for modeling
score differential… but that didn’t tell us anything interesting. What we’re
really interested in, is the relationship between some predictor variable(s) and
a team’s score differential. For instance, how is a team’s passing performance
related to their score differential? What about their pass defense? Is this more
or less related to score differential than rushing?&lt;/p&gt;
&lt;p&gt;In order to address these questions, I’m going to define two statistics to
summarise each team’s performance in a season. These statistics will be entirely
based on a team’s expected points added (&lt;span class=&#34;math inline&#34;&gt;\(EPA\)&lt;/span&gt;). If you’re not familiar with &lt;span class=&#34;math inline&#34;&gt;\(EPA\)&lt;/span&gt;,
it’s built on the fundamental principle in football that not all yards are created
equal. A 3 yard rush on 3rd down with 2 yards to go, is a lot more valuable than
a 3 yard rush on 3rd down with 15 yards to go. I’m not going to do a full dive
on how to calculate &lt;a href=&#34;https://www.cmusportsanalytics.com/nfl-expected-points-nflscrapr-part-1-introduction-expected-points/&#34;&gt;expected points or its history in this post&lt;/a&gt;,
but it gives us a number describing how many points a team is expected to score
given their current situation. &lt;span class=&#34;math inline&#34;&gt;\(EPA\)&lt;/span&gt; is merely the change in this expected points
state between two plays. It was recently popularized by &lt;a href=&#34;http://www.advancedfootballanalytics.com/index.php/home/stats/stats-explained/expected-points-and-epa-explained&#34;&gt;Brian Burke&lt;/a&gt;,
and in &lt;a href=&#34;https://github.com/maksimhorowitz/nflscrapR&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;&lt;/a&gt; we use a
multinomial logistic regression model to provide probabilities for each of the
possible scoring events resulting in freely available &lt;span class=&#34;math inline&#34;&gt;\(EPA\)&lt;/span&gt; values for every
play, &lt;a href=&#34;https://arxiv.org/abs/1802.00998&#34;&gt;full model details available here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For each combination of team, &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt;, and season, &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt;, we’ll calculate both its
total offensive &lt;span class=&#34;math inline&#34;&gt;\(EPA\)&lt;/span&gt; from passing and rushing as &lt;span class=&#34;math inline&#34;&gt;\(EPA^{pass}_{off, t, y}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(EPA^{rush}_{off, t, y}\)&lt;/span&gt; respectively. As well as it’s defensive statistics
representing the expected points &lt;strong&gt;allowed&lt;/strong&gt;, &lt;span class=&#34;math inline&#34;&gt;\(EPA^{pass}_{def, t, y}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(EPA^{rush}_{def, t, y}\)&lt;/span&gt;. We’ll capture the efficiency of their performances
by dividing these totals each by their respective number of attempts, which we’ll
denote as &lt;span class=&#34;math inline&#34;&gt;\(Att^{rush}_{off, t,y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Att^{rush}_{def, t,y}\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(Att^{pass}_{off, t,y}\)&lt;/span&gt;,
and &lt;span class=&#34;math inline&#34;&gt;\(Att^{pass}_{def, t,y}\)&lt;/span&gt;. Finally, we’ll summarise their overall performances
with differentials between their offensive and defensive efficiency stats for
both passing and rushing:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
EPA\  diff^{pass}_{t,y} = \frac{EPA^{pass}_{off, t, y}}{Att^{pass}_{off, t,y}} - \frac{EPA^{pass}_{def, t, y}}{Att^{pass}_{def, t,y}}\\ 
EPA\  diff^{rush}_{t,y} = \frac{EPA^{rush}_{off, t, y}}{Att^{rush}_{off, t,y}} - \frac{EPA^{rush}_{def, t, y}}{Att^{rush}_{def, t,y}}\\
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;These two stats tell us effectively tell us how much more efficient a team’s
passing (rushing) offense was compared to their opponents. We’re then going to
look at the relationship between score differential and each of these efficiency
differentials separately for simplicity to give us a sense of knowing which
is more related to their overall performance as captured by score differential.
First let’s get this data and join it to our &lt;code&gt;team_summary_df&lt;/code&gt; above by grabbing
and selecting the columns from my &lt;a href=&#34;https://ryurko.github.io/nflscrapR-data/&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;-data repository&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load in the four datasets summarising team performance for passing and rushing
# both from offensive and defensive perspectives, selecting only the necessary
# columns and renaming them:

team_passing_off &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/data/season_team_stats/team_season_passing_df.csv&amp;quot;) %&amp;gt;%
  dplyr::select(Season, Team, EPA_per_Att) %&amp;gt;%
  rename(pass_epa_att_off = EPA_per_Att) %&amp;gt;%
  filter(!is.na(Team))

team_passing_def &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/data/season_team_stats/team_def_season_passing_df.csv&amp;quot;) %&amp;gt;%
  dplyr::select(Season, Team, EPA_per_Att) %&amp;gt;%
  rename(pass_epa_att_def = EPA_per_Att) %&amp;gt;%
  filter(!is.na(Team))

team_rushing_off &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/data/season_team_stats/team_season_rushing_df.csv&amp;quot;) %&amp;gt;%
  dplyr::select(Season, Team, EPA_per_Car) %&amp;gt;%
  rename(rush_epa_att_off = EPA_per_Car) %&amp;gt;%
  filter(!is.na(Team))

team_rushing_def &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/data/season_team_stats/team_def_season_rushing_df.csv&amp;quot;) %&amp;gt;%
  dplyr::select(Season, Team, EPA_per_Car) %&amp;gt;%
  rename(rush_epa_att_def = EPA_per_Car) %&amp;gt;%
  filter(!is.na(Team))

# Join the data to the team_summary_df and calculate the differential columns:

team_summary_df &amp;lt;- team_summary_df %&amp;gt;%
  inner_join(team_passing_off, by = c(&amp;quot;Team&amp;quot;, &amp;quot;Season&amp;quot;)) %&amp;gt;%
  inner_join(team_passing_def, by = c(&amp;quot;Team&amp;quot;, &amp;quot;Season&amp;quot;)) %&amp;gt;%
  inner_join(team_rushing_off, by = c(&amp;quot;Team&amp;quot;, &amp;quot;Season&amp;quot;)) %&amp;gt;%
  inner_join(team_rushing_def, by = c(&amp;quot;Team&amp;quot;, &amp;quot;Season&amp;quot;)) %&amp;gt;%
  mutate(pass_epa_diff = pass_epa_att_off - pass_epa_att_def,
         rush_epa_diff = rush_epa_att_off - rush_epa_att_def)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, let’s look at the relationship between each of these efficiency differentials
with the team’s score differential visually along with simple linear regression
fits (just for assistance, we’re going into Bayesian linear regression anyway):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;epa_pass_plot &amp;lt;- ggplot(team_summary_df,
                        aes(x = pass_epa_diff, y = Total_Score_Diff)) +
  geom_point(alpha = 0.5, color = &amp;quot;darkblue&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = TRUE) +
  labs(x = &amp;quot;Passing EPA/Attempt differential&amp;quot;,
       y = &amp;quot;Regular season score differential&amp;quot;,
       title = &amp;quot;Relationship between score differential\nand passing efficiency differential&amp;quot;) +
  theme_bw() + 
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 13))

epa_rush_plot &amp;lt;- ggplot(team_summary_df,
                        aes(x = rush_epa_diff, y = Total_Score_Diff)) +
  geom_point(alpha = 0.5, color = &amp;quot;darkblue&amp;quot;) +
  geom_smooth(method = &amp;quot;lm&amp;quot;, se = TRUE) +
  labs(x = &amp;quot;Rushing EPA/Attempt differential&amp;quot;,
       y = &amp;quot;Regular season score differential&amp;quot;,
       title = &amp;quot;Relationship between score differential\n and rushing efficiency differential&amp;quot;) +
  theme_bw() + 
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 13))

plot_grid(epa_pass_plot, epa_rush_plot, rel_widths = c(1, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;864&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Ok, I’d say it’s pretty obvious that the passing efficiency differential has
a strong positive relationship with score differential. The rushing differential
also displays a positive relationship, but not quite as strong. We can also
of course view the relationship between these variables (or lack thereof), and
color the points by the team’s regular season score differential to give us an
idea of how its related to both of these variables. And just because we can,
we’ll include the marginal distributions on the same plot.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First generate the scattterplot with the points
joint_score_plot &amp;lt;- ggplot(team_summary_df,
       aes(x = pass_epa_diff, y = rush_epa_diff)) +
  geom_point(aes(color = Total_Score_Diff), alpha = 0.5, size = 4) +
  labs(x = &amp;quot;Passing EPA/Attempt differential&amp;quot;,
       y = &amp;quot;Rushing EPA/Attempt differential&amp;quot;,
       color = &amp;quot;Regular season\nscore differential&amp;quot;,
       title = &amp;quot;Joint distribution of passing and rushing efficiency differentials\ncolored by regular season score differential&amp;quot;) +
  scale_color_gradient2(low = &amp;quot;darkorange&amp;quot;, mid = &amp;quot;gray&amp;quot;, high = &amp;quot;darkblue&amp;quot;,
                        midpoint = 0) +
  geom_vline(xintercept = 0, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;) +
  geom_hline(yintercept = 0, linetype = &amp;quot;dashed&amp;quot;, color = &amp;quot;red&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Better pass,\nbetter run&amp;quot;, x = .30, y = .28,
           size = 5, color = &amp;quot;darkred&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Better pass,\nworse run&amp;quot;, x = .30, y = -.15,
           size = 5, color = &amp;quot;darkred&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Worse pass,\nbetter run&amp;quot;, x = -.30, y = .28,
           size = 5, color = &amp;quot;darkred&amp;quot;) +
  annotate(&amp;quot;text&amp;quot;, label = &amp;quot;Worse pass,\nworse run&amp;quot;, x = -.30, y = -.15,
           size = 5, color = &amp;quot;darkred&amp;quot;) +
  theme_bw() + 
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 16))


pass_dens &amp;lt;- axis_canvas(joint_score_plot, axis = &amp;quot;x&amp;quot;) +
  geom_density(data = team_summary_df, aes(x = pass_epa_diff), 
               fill = &amp;quot;darkblue&amp;quot;,
               alpha = 0.5, size = .2)

# Same thing for sigma but along y and with coord_flip = TRUE to make it vertical:
rush_dens &amp;lt;- axis_canvas(joint_score_plot, axis = &amp;quot;y&amp;quot;, coord_flip = TRUE) +
  geom_density(data = team_summary_df, aes(x = rush_epa_diff), 
               fill = &amp;quot;darkblue&amp;quot;,
               alpha = 0.5, size = .2) +
  coord_flip()

# Now generate by adding these objects to the main plot:
joint_score_plot_1 &amp;lt;- insert_xaxis_grob(joint_score_plot, pass_dens, 
                                  grid::unit(.2, &amp;quot;null&amp;quot;), position = &amp;quot;top&amp;quot;)
joint_score_plot_2 &amp;lt;- insert_yaxis_grob(joint_score_plot_1, rush_dens, 
                                  grid::unit(.2, &amp;quot;null&amp;quot;), position = &amp;quot;right&amp;quot;)
ggdraw(joint_score_plot_2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now it looks pretty clear that you’re probably going to need a better passing offense
than your opponent to have a positive score differential. There doesn’t appear
to be many points on this plot with positive score differential in the top-left
quadrant, which corresponds to rushing better than your opponents but having an
inferior passing game. After accounting for the passing efficiency differential,
maybe a team’s rushing efficiency differential doesn’t matter? Let’s tackle this
with a Bayesian model.&lt;/p&gt;
&lt;p&gt;Returning to the score differential model, we’re now going use both efficiency
differentials as predictors. Remember, all we need to fully describe a Gaussian
distribution is the mean and variance. So in order to incorporate these into
the our model we’ll define the mean, &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt;, as a function of both predictors
explicitly. For simplicity we’ll return to using &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt; to denote a single combination
of team &lt;span class=&#34;math inline&#34;&gt;\(t\)&lt;/span&gt; and season &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for the 286 team-season combinations
that we have, and we’ll also let &lt;span class=&#34;math inline&#34;&gt;\(P_i = EPA\  diff^{pass}_{t,y}\)&lt;/span&gt; and
&lt;span class=&#34;math inline&#34;&gt;\(R_i = EPA\  diff^{rush}_{t,y}\)&lt;/span&gt; be simpler representations for our differentials:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_i \sim \text{N}(\mu_i, \sigma^2) \\
\mu_i = \alpha + \beta_{P}P_i + \beta_R R_i \\
\alpha \sim \text{N}(0, 100^2) \\
\beta_P \sim \text{N}(0, 100^2) \\
\beta_R \sim \text{N}(0, 100^2) \\
\sigma \sim \text{Uniform}(0,200)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Now the mean of the score differential distribution &lt;em&gt;depends on the predictors&lt;/em&gt;
as denoted with the subscript &lt;span class=&#34;math inline&#34;&gt;\(i\)&lt;/span&gt;. By denoting the relationship between &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt;
and each of the predictors with an &lt;span class=&#34;math inline&#34;&gt;\(=\)&lt;/span&gt; sign, we’re saying that &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; is no
longer a parameter, its just a function of our actual parameters of interest.
The parameters we really care about here are &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;\(\beta_P\)&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;\(\beta_R\)&lt;/span&gt;.
Of course all of this should look familiar if you’ve seen &lt;a href=&#34;https://en.wikipedia.org/wiki/Linear_regression&#34;&gt;linear regression&lt;/a&gt;
before. The &lt;span class=&#34;math inline&#34;&gt;\(\alpha\)&lt;/span&gt; is our model’s intercept, representing the &lt;em&gt;expected&lt;/em&gt; score
differential for when both the passing and rushing efficiency differentials
are 0. And each of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s represent the change in &lt;em&gt;expected&lt;/em&gt; score
differential when the pass/rush efficiency differential increases by 1 unit, after
accounting for the other type of efficiency differential (rush/pass). So each of
these are given priors (along with &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; again), and we don’t need to specify
a prior for &lt;span class=&#34;math inline&#34;&gt;\(\mu_i\)&lt;/span&gt; since it is explained completely by these parameters. These
priors for now are pretty weak, but both &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; priors centered at 0 are just
saying, “I don’t know if there’s a relationship between these effiency
differentials and score differential, good or bad are equally likely”.&lt;/p&gt;
&lt;p&gt;To actually approximate this model, we’ll use that same &lt;code&gt;laplace_approx()&lt;/code&gt; function
from before - but this time define a new function for calculating the log posterior
for this regression model:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#&amp;#39; Function to calculate log posterior for score differential model regression
#&amp;#39; model that also takes in the prior inputs for the regression parameters.
#&amp;#39; @param reg_params Vector of parameter values that are named &amp;quot;alpha&amp;quot;, 
#&amp;#39; &amp;quot;beta_p&amp;quot;, &amp;quot;beta_r&amp;quot;, and &amp;quot;sigma&amp;quot;.
#&amp;#39; @param hyperparams List of hyperparameter values that are named &amp;quot;alpha_mu&amp;quot;,
#&amp;#39; &amp;quot;alpha_sd&amp;quot;, &amp;quot;beta_p_mu&amp;quot;, &amp;quot;beta_p_sd&amp;quot;, &amp;quot;beta_r_mu&amp;quot;, &amp;quot;beta_p_sd&amp;quot;, &amp;quot;sigma_a&amp;quot;,
#&amp;#39; and &amp;quot;sigma_b&amp;quot;.
#&amp;#39; @param score_diff_values Vector of observed score differential values.
#&amp;#39; @param pass_eff_values Vector of pass efficiency differential values.
#&amp;#39; @param rush_eff_values Vector of rush efficiency differential values.

score_diff_reg_eff_model &amp;lt;- function(reg_params, hyperparams, 
                                        score_diff_values, pass_eff_values,
                                        rush_eff_values) {
  # Log likelihood that now uses the function form for mu using the two
  # predictors and their respective values:
  sum(dnorm(score_diff_values, 
            reg_params[&amp;quot;alpha&amp;quot;] + 
              reg_params[&amp;quot;beta_p&amp;quot;]*pass_eff_values +
              reg_params[&amp;quot;beta_r&amp;quot;]*rush_eff_values, 
            reg_params[&amp;quot;sigma&amp;quot;], log = TRUE)) +
    # plus the log priors for each parameter results in log posterior:
    dnorm(reg_params[&amp;quot;alpha&amp;quot;], 
          hyperparams$alpha_mu, hyperparams$alpha_sd, log = TRUE) + 
    dnorm(reg_params[&amp;quot;beta_p&amp;quot;], 
          hyperparams$beta_p_mu, hyperparams$beta_p_sd, log = TRUE) + 
    dnorm(reg_params[&amp;quot;beta_r&amp;quot;], 
          hyperparams$beta_r_mu, hyperparams$beta_r_sd, log = TRUE) + 
    dunif(reg_params[&amp;quot;sigma&amp;quot;], hyperparams$sigma_a, 
          hyperparams$sigma_b, log = TRUE)

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we’ll just optimize this function in the same way as before, using the
hyperparameters in the model outlined above, and generate 10000 samples from the
resulting multivariate Gaussian posterior distribution:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;init_reg_samples &amp;lt;- laplace_approx(log_posterior_fn = score_diff_reg_eff_model,
                               init_points = c(alpha = 0, beta_p = 0, 
                                               beta_r = 0, sigma = 10),
                               n_samples = 10000,
                               # Vector of hyperparameters:
                               hyperparams = list(alpha_mu = 0, alpha_sd = 100,
                                                beta_p_mu = 0, beta_p_sd = 100,
                                                beta_r_mu = 0, beta_r_sd = 100,
                                                sigma_a = 0, sigma_b = 200),
                               # Specify the data for the optimization!
                               score_diff_values = team_summary_df$Total_Score_Diff,
                               pass_eff_values = team_summary_df$pass_epa_diff,
                               rush_eff_values = team_summary_df$rush_epa_diff)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s view the distributions for each &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; using another
one of &lt;a href=&#34;https://cran.r-project.org/web/packages/ggridges/vignettes/introduction.html&#34;&gt;Claus Wilke’s packages &lt;code&gt;ggridges&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# We&amp;#39;ll use the latex2exp package here:
# install.packages(&amp;quot;latex2exp&amp;quot;)
library(latex2exp)
# and ggridges:
# install.packages(&amp;quot;ggridges&amp;quot;) 
library(ggridges)

# First use the gather function to make this simpler to work with:
init_reg_samples %&amp;gt;%
  gather(param, value) %&amp;gt;%
  # only use the beta parameters:
  filter(param %in% c(&amp;quot;beta_p&amp;quot;, &amp;quot;beta_r&amp;quot;)) %&amp;gt;%
  # visualize the distributions for each with density curves:
  ggplot(aes(x = value, y = param)) +
  geom_density_ridges(alpha = 0.7, fill = &amp;quot;darkblue&amp;quot;,
                      # add the rugs underneath as well:
                      jittered_points = TRUE,
                      position = position_points_jitter(width = 0.05, height = 0),
                      point_shape = &amp;#39;|&amp;#39;, point_size = 3, point_color = &amp;quot;darkblue&amp;quot;,
                      point_alpha = 0.7) +
  # properly label and also change the y axis spacing so the Passing density
  # is along the bottom of the axis
  scale_y_discrete(labels = c(&amp;quot;Passing&amp;quot;, &amp;quot;Rushing&amp;quot;), expand = c(0.01, 0.01)) +
  # label using the latex symbols:
  labs(x = TeX(&amp;quot;$\\beta$ value&amp;quot;),
       y = TeX(&amp;quot;$\\beta$ parameter&amp;quot;),
       title = TeX(&amp;quot;Sampled posterior distributions for $\\beta_P$ and $\\beta_R$&amp;quot;)) +
  # theme settings:
  theme_bw() + 
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;From this we can clearly see that both passing and rushing efficiency differentials
have positive relationships with score differential, even while accounting for
each other. Unlike my initial guess from the previous plot, improving your rushing
efficiency differential is associated with an increase in your team’s score
differential even after adjusting for passing - so it’s not entirely useless.
But we do see that passing has a greater effect, in fact &lt;strong&gt;there’s no overlap
between the passing and rushing distributions&lt;/strong&gt;.
Now these &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt; values are quite large due to the actual values
for the &lt;span class=&#34;math inline&#34;&gt;\(EPA/Att\)&lt;/span&gt; differentials, with the medians for &lt;span class=&#34;math inline&#34;&gt;\(\beta_P\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\beta_R\)&lt;/span&gt; at
490.448 and
315.0773 respectively. To make this
simpler and more realistic from point of view of gaining an advantage: a 0.05
increase in a team’s passing &lt;span class=&#34;math inline&#34;&gt;\(EPA/Att\)&lt;/span&gt; differential is associated with a
24.5224 increase in their expected
score differential (using the posterior distribution median). Meanwhile a 0.05
increase in a team’s rushing &lt;span class=&#34;math inline&#34;&gt;\(EPA/Att\)&lt;/span&gt; differential is associated with a
15.7539 increase in their expected
score differential. So if you’re in a front office and wondering how to allocate
your resources, improving your pass offense/defense is more worthwhile
than improving in your run offense/defense.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-my-prior-says-run-the-ball&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;But my prior says run the ball!&lt;/h2&gt;
&lt;p&gt;I completely glossed over the prior distributions in setting up this model.
The ones I used above are really weak priors, that give you results
pretty close to just using good old &lt;code&gt;lm()&lt;/code&gt; for your standard linear regression.
Also, the prior I’m using for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt; is extremely naive, a more appropriate
choice is to actually model the log(&lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;) instead with a Gaussian prior then
exponeniate to get strictly positive values - but I’ll let the motivated reader
adjust this. Instead I’m going to conclude this post with an example of the power
of prior distributions.&lt;/p&gt;
&lt;p&gt;Let’s pretend you’re an analyst for the Seattle Seahawks. Your team has just
hired &lt;a href=&#34;https://en.wikipedia.org/wiki/Brian_Schottenheimer&#34;&gt;Brian Schottenheimer&lt;/a&gt;
to be the offensive coordinator. Brian loves to run the football&lt;a href=&#34;#fn5&#34; class=&#34;footnote-ref&#34; id=&#34;fnref5&#34;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;, in fact he tells
you that from his 20 years of professional coaching experience, he knows that
the run game is much more important than the passing game. So how do we account
for his prior knowledge (or to be blunt, his bias)?&lt;/p&gt;
&lt;p&gt;A pretty cool feature from using the Gaussian distribution as a prior is how
the variance represents the amount of previous data from using &lt;span class=&#34;math inline&#34;&gt;\(\sigma = \frac{1}{\sqrt{n}}\)&lt;/span&gt;.
Given Brian’s 20 years of professional experience, this equates to using in our
priors $=  $ 0.2236,
which is obviously much smaller than the priors from above. We’ll also specify
the &lt;span class=&#34;math inline&#34;&gt;\(\mu\)&lt;/span&gt; for each of the &lt;span class=&#34;math inline&#34;&gt;\(\beta\)&lt;/span&gt;s to say that rushing has a stronger relationship
than passing, by just flipping our posterior median values in the example above:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
S_i \sim \text{N}(\mu_i, \sigma^2) \\
\mu_i = \alpha + \beta_{P}P_i + \beta_R R_i \\
\alpha \sim \text{N}(0, 100^2) \\
\beta_P \sim \text{N}(336, 0.2236^2) \\
\beta_R \sim \text{N}(492, 0.2236^2) \\
\sigma \sim \text{Uniform}(0,200)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let’s use our functions from before and visualize the new posteriors given
Brian’s prior beliefs:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;brian_reg_samples &amp;lt;- laplace_approx(log_posterior_fn = score_diff_reg_eff_model,
                               init_points = c(alpha = 0, beta_p = 0, 
                                               beta_r = 0, sigma = 10),
                               n_samples = 10000,
                               # Vector of hyperparameters:
                               hyperparams = list(alpha_mu = 0, alpha_sd = 100,
                                                beta_p_mu = 336, beta_p_sd = 0.2236,
                                                beta_r_mu = 492, beta_r_sd = 0.2236,
                                                sigma_a = 0, sigma_b = 200),
                               # Specify the data for the optimization!
                               score_diff_values = team_summary_df$Total_Score_Diff,
                               pass_eff_values = team_summary_df$pass_epa_diff,
                               rush_eff_values = team_summary_df$rush_epa_diff)

# First use the gather function to make this simpler to work with:
brian_reg_samples %&amp;gt;%
  gather(param, value) %&amp;gt;%
  # only use the beta parameters:
  filter(param %in% c(&amp;quot;beta_p&amp;quot;, &amp;quot;beta_r&amp;quot;)) %&amp;gt;%
  # visualize the distributions for each with density curves:
  ggplot(aes(x = value, y = param)) +
  geom_density_ridges(alpha = 0.7, fill = &amp;quot;darkblue&amp;quot;,
                      # add the rugs underneath as well:
                      jittered_points = TRUE,
                      position = position_points_jitter(width = 0.05, height = 0),
                      point_shape = &amp;#39;|&amp;#39;, point_size = 3, point_color = &amp;quot;darkblue&amp;quot;,
                      point_alpha = 0.7) +
  # properly label and also change the y axis spacing so the Passing density
  # is along the bottom of the axis
  scale_y_discrete(labels = c(&amp;quot;Passing&amp;quot;, &amp;quot;Rushing&amp;quot;), expand = c(0.01, 0.01)) +
  # label using the latex symbols:
  labs(x = TeX(&amp;quot;$\\beta$ value&amp;quot;),
       y = TeX(&amp;quot;$\\beta$ parameter&amp;quot;),
       title = TeX(&amp;quot;Sampled posterior distributions for $\\beta_P$ and $\\beta_R$ accounting for Brian&amp;#39;s prior&amp;quot;)) +
  # theme settings:
  theme_bw() + 
  theme(axis.text = element_text(size = 10),
        axis.title = element_text(size = 12),
        plot.title = element_text(size = 14))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-07-04-bayesian-baby-steps-normal-next-steps_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This shouldn’t be a suprise, as those were some very strong priors about the
relationships for each of these efficiency differentials. Priors play an
important role in Bayesian data analysis precisely for this reason, they can
have a major impact on your results (hence why many people prefer to avoid
working with them). So if you’re that lucky Seahawks analyst that
gets to show Brian these results, you’ve just reaffirmed his beliefs - you’ll
need a lot more data to move these posterior distributions away from such strong priors.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;discussion-and-next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Discussion and next steps&lt;/h2&gt;
&lt;p&gt;Laplace approximation is a useful tool that is pretty easy to implement, assuming
you’ve met the necessary assumptions. You’re not just using the mode found from
optimization techniques as single estimates, but rather approximating the full
posterior distribution with a Gaussian. And from this score differential example,
it’s really easy to implement Bayesian linear regression. From the football
perspective we’ve seen that the ability to pass the football and prevent others
from passing is more valuable in terms of score differential than running the ball.
But strong priors can lead us to basically ignore the data, just confirming to
someone like Brian Schottenheimer that his personal bias is “correct”. Obviously
this analysis was pretty simple, and much more work would need to be done for
really assessing the importance of passing and rushing in the NFL. But even
with this simple approach, &lt;strong&gt;we can quantify in terms of points how much more
valuable passing is relative to rushing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The next post in this series is going to focus on sampling from the posterior
distribution - something I’ve done multiple times but deserves more of deep
dive on its own before moving into MCMC.&lt;/p&gt;
&lt;p&gt;Let me know if there are any errors in this post/code, all feedback is welcome!&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media2.giphy.com/media/E0RLYXkGjsYqQ/giphy-downsized.gif?cid=e1bb72ff5b3c51776d6e384d5955e0eb&#34; alt=&#34;You actually read this whole thing?&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;You actually read this whole thing?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Richard McElreath’s &lt;a href=&#34;https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445&#34;&gt;Statistical Rethinking&lt;/a&gt;,
his &lt;a href=&#34;https://github.com/rmcelreath/rethinking&#34;&gt;&lt;code&gt;rethinking&lt;/code&gt; package&lt;/a&gt; uses a
function &lt;code&gt;map()&lt;/code&gt;, which is basically a wrapper function for
the &lt;code&gt;laplace_approx()&lt;/code&gt; function we defined above, to carry out Laplace approximation. I’m
just personally annoyed by the fact it’s &lt;code&gt;map()&lt;/code&gt; since that’s the same name as
one of the most useful functions in the &lt;code&gt;purrr&lt;/code&gt; package.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954&#34;&gt;Bayesian Data Analysis&lt;/a&gt;
by Gelman et al.&lt;/li&gt;
&lt;li&gt;Rasmus Bååth’s &lt;a href=&#34;http://www.sumsar.net/blog/2013/11/easy-laplace-approximation/&#34;&gt;blog post on Laplace approximation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jim Albert’s &lt;a href=&#34;https://cran.r-project.org/web/packages/LearnBayes/LearnBayes.pdf&#34;&gt;&lt;code&gt;LearnBayes&lt;/code&gt; package&lt;/a&gt;
also has similar functions for carrying out this approximation.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;My rocket scientist - turned nuclear engineer - turned self taught Bayesian
data scientist for a brother loves Laplace approximation.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Ridiculously bad but awesome idea - Lin-Manuel Miranda reads
Stephen Stigler’s &lt;a href=&#34;https://www.amazon.com/History-Statistics-Measurement-Uncertainty-before/dp/067440341X&#34;&gt;History of Statistics&lt;/a&gt; and creates a Broadway musical with Daveed Diggs as Laplace…&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;I don’t see the point of ever referring to something by its Latin name,
so I’m going to call it what it is: the posterior mode.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn4&#34;&gt;&lt;p&gt;Shameless self promotion for my football data package &lt;a href=&#34;https://github.com/ryurko/fcscrapR&#34;&gt;&lt;code&gt;fcscrapR&lt;/code&gt;&lt;/a&gt;!&lt;a href=&#34;#fnref4&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn5&#34;&gt;&lt;p&gt;I’m sorry &lt;a href=&#34;https://twitter.com/benbbaldwin&#34;&gt;Ben&lt;/a&gt; and &lt;a href=&#34;https://twitter.com/SeanFromSeabeck&#34;&gt;Sean&lt;/a&gt;.&lt;a href=&#34;#fnref5&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Baby Steps: Intro with JuJu</title>
      <link>http://www.stat.cmu.edu/~ryurko/post/bayesian-baby-steps-intro/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.stat.cmu.edu/~ryurko/post/bayesian-baby-steps-intro/</guid>
      <description>&lt;div id=&#34;first-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;First steps&lt;/h2&gt;
&lt;p&gt;I was originally thinking of writing a blog post about multilevel models (aka
hierachical, mixed, random effects) because of how useful they are for measuring
player performance in sports&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; (&lt;a href=&#34;https://arxiv.org/abs/1802.00998&#34;&gt;shameless self promotion for nflWAR here!&lt;/a&gt;).
But the more I thought about it, the more I realized how ill-minded of an idea
that was. Instead, I want to build up the intuition for how and why one would
want to use a full Bayesian multilevel model. My brother recommended that I checkout &lt;a href=&#34;https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445&#34;&gt;Statistical Rethinking&lt;/a&gt;
by &lt;a href=&#34;http://xcelab.net/rm/&#34;&gt;Richard McElreath&lt;/a&gt;. It’s an amazing introduction to
Bayesian data analysis that I recommend to anyone interested in learning more.
It’s incredibly intuitive and paced very well, for instance MCMC isn’t covered
until halfway through the book so you understand why you’re using it.
Taking what I have learned from the book, I’m writing a series of posts that will
hopefully build the basic understanding of how Bayesian inference works starting
with simple updating and gradually working up towards multilevel modeling. This
series is called Bayesian Baby Steps for a reason, and there will be &lt;em&gt;What About Bob?&lt;/em&gt;
GIFs. Because of my work with &lt;a href=&#34;https://github.com/maksimhorowitz/nflscrapR&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;&lt;/a&gt;,
all my examples will be using NFL data which is ripe for Bayesian data analysis.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media2.giphy.com/media/Hg3LAJ9i9yCic/giphy-downsized.gif?cid=e1bb72ff5b0f54d23643475745f46c52&#34; alt=&#34;Bayes Bob!&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Bayes Bob!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;catch-rate-example&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Catch rate example&lt;/h2&gt;
&lt;p&gt;You’re trying to evaluate a receiver’s ability to catch a football. Let’s
pretend you can take the following (completely unrealistic) strategy: you tell
your quarterback to repeatedly throw the ball to your receiver in practice,
recording each time whether or not they caught the ball. We’ll let C stand for
catch and D stand for drop. We could carry out this procedure any number of
times, for instance we could record the following ten pass attempts to our
receiver:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sample(c(&amp;quot;C&amp;quot;, &amp;quot;D&amp;quot;), size = 10, replace = TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;D&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot; &amp;quot;D&amp;quot; &amp;quot;D&amp;quot; &amp;quot;C&amp;quot; &amp;quot;C&amp;quot; &amp;quot;D&amp;quot; &amp;quot;C&amp;quot; &amp;quot;C&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can describe the &lt;em&gt;story&lt;/em&gt; for this &lt;em&gt;data generating process&lt;/em&gt; quite easily,
informing us how to simulate new data. In this case:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Our receiver’s true catch rate is &lt;code&gt;p&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Each single pass attempt has a probability &lt;code&gt;p&lt;/code&gt; of being caught, thus meaning
a pass has a probability of &lt;code&gt;1 - p&lt;/code&gt; of being dropped by our receiver.&lt;/li&gt;
&lt;li&gt;We assume each pass attempt is independent of one another.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’re now ready to explore this probability model and how to handle observing
data to update our beliefs about our receiver’s catch rate &lt;code&gt;p&lt;/code&gt; using Bayesian
inference.&lt;/p&gt;
&lt;p&gt;In a Bayesian model, there are three things we need to choose to ultimately
represent the number of ways our data can be generated:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Likelihood - plausibility of our observed data given a receiver’s catch rate
&lt;code&gt;p&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Parameter - our quantity of interest &lt;code&gt;p&lt;/code&gt;, which we want to learn about from
our data&lt;/li&gt;
&lt;li&gt;Prior - our initial belief regarding different values for &lt;code&gt;p&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Based on the &lt;em&gt;story&lt;/em&gt; above, it’s pretty easy to see that we can just use the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_distribution&#34;&gt;binomial distribution&lt;/a&gt;
for our likelihood since we’re assuming each of our &lt;code&gt;n&lt;/code&gt; pass attempts is
independent and that the receiver’s catch probability &lt;code&gt;p&lt;/code&gt; is the same for every
attempt. We can then write the probability of observing &lt;code&gt;c&lt;/code&gt; receptions in
&lt;code&gt;n&lt;/code&gt; pass attempts with a catch probability &lt;code&gt;p&lt;/code&gt; as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Pr}(c | n, p) = \frac{n!}{c!(n - c)!}p^c (1 - p)^{n-c}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;We then use good old &lt;a href=&#34;https://en.wikipedia.org/wiki/Bayes%27_theorem&#34;&gt;Bayes’ theorem&lt;/a&gt;
to provide us with the probability of a value for the receiver’s catch
probability &lt;code&gt;p&lt;/code&gt; given our observed data:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Pr}(p | c) = \frac{\text{Pr}(c|p) \text{Pr}(p)}{\text{Pr}(c)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I personally like the way McElreath refers to the denonimator,
&lt;span class=&#34;math inline&#34;&gt;\(\text{Pr}(c)\)&lt;/span&gt;, as the &lt;em&gt;average&lt;/em&gt; likelihood of the observed data over &lt;code&gt;p&lt;/code&gt;. All
it is is an expectation over all our possible values for &lt;code&gt;p&lt;/code&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\text{Pr}(c) = \text{E}\big[\text{Pr}(c|p)\big] = \int \text{Pr}(c|p) \text{Pr}(p)dp
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This denonimator is known as the &lt;em&gt;normalizing constant&lt;/em&gt;
because it ensures that our posterior density integrates to one, i.e. our
posterior is actually a probability. What often happens is that this
term is extremely difficult to compute. We usually consider the
posterior up to the normalizing constant as the product between the likelihood
and prior. Or if this form isn’t tractable, we resort to some approximation
technique such as grid-based which we cover below, as well as quadratic and Markov
Chain Monte Carlo (MCMC) methods. However we’ll cover those in a post later,
remember baby steps people!&lt;/p&gt;
&lt;p&gt;One notion that I completely glossed over above is how McElreath motivates the
mechanism for Bayesian analysis. He provides a walkthrough of the &lt;em&gt;garden of
forking data&lt;/em&gt; describing how, in formulating the posterior above, we’re really
just counting paths. We simply use multiplication as a quick way to count all
the ways from our prior number of paths through our new number. McElreath also
makes the excellent remark that &lt;strong&gt;Bayesian inference is not defined by Bayes’
theorem&lt;/strong&gt;. Everyone learns about Bayes theorem with some trivial examples like
&lt;a href=&#34;http://www.milefoot.com/math/stat/prob-bayes.htm&#34;&gt;smoking and lung cancer&lt;/a&gt;,
but that is not the point of Bayesian approaches. The point is to &lt;strong&gt;appropriately
measure and account for the uncertainty in our models and parameters&lt;/strong&gt;. We’re not
satisfied with stating, “Oh our receiver caught 5 of 5 passes - he never drops
a pass!” - we want to capture the uncertainty in his reception probability
given what we already knew, such as the belief it’s pretty unlikely his catch
rate is a perfect 100%.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;grid-approximation-with-juju&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Grid approximation with JuJu&lt;/h2&gt;
&lt;p&gt;Even though from probability theory we know that every combination of prior,
likelihood, and data constitutes a unique posterior - in many cases we cannot
derive the resulting posterior. This leads to the use of approximation
techniques such as grid approximation, quadratic approximation, and MCMC to
name a few. In this post we’ll cover grid approximation because of its
simplicity, which makes it a great way to learn the basics behind Bayesian
updating.&lt;/p&gt;
&lt;p&gt;Let’s expand on our catch rate model a bit by going back in time, and assume
we’re working the Pittsburgh Steelers heading into the 2017 NFL season. We
drafted JuJu Smith-Schuster to add another option to the offense and want to
evaluate his performance as the season goes on based on his catch rate.
Using the &lt;a href=&#34;https://github.com/maksimhorowitz/nflscrapR&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;&lt;/a&gt; package we
can easily get all plays from the 2017 season. You can use the &lt;code&gt;season_play_by_play()&lt;/code&gt;
function from the package to scrape the data, or you can access the files I
saved here in my &lt;a href=&#34;https://ryurko.github.io/nflscrapR-data/&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;-data repository&lt;/a&gt;.
The following code will access the 2017 play-by-play data, filter down only
to the pass attempts to JuJu, then create a dataset that summarizes his
performance in each game he played along with cumulative totals after each game:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Access the tidyverse (install if you don&amp;#39;t have it!)
# install.packages(&amp;quot;tidyverse&amp;quot;)
library(tidyverse)

# Load the 2017 play-by-play data from my repository:
juju_games &amp;lt;- 
  read_csv(&amp;quot;https://raw.githubusercontent.com/ryurko/nflscrapR-data/master/data/season_play_by_play/pbp_2017.csv&amp;quot;) %&amp;gt;%
  # Filter down only to the pass attempts to JuJu based on his GSIS ID 00-0033857:
  filter(Receiver_ID == &amp;quot;00-0033857&amp;quot;,
         PassAttempt == 1) %&amp;gt;%
  # Only select the GameID and PassOutcome columns:
  select(GameID, PassOutcome) %&amp;gt;%
  # Calculate the number of receptions, targets, and catch rate in each game:
  group_by(GameID) %&amp;gt;%
  summarise(receptions = length(which(PassOutcome == &amp;quot;Complete&amp;quot;)),
            targets = n(),
            catch_rate = receptions / targets) %&amp;gt;%
  # Calculate cumulative stats:
  mutate(total_receptions = cumsum(receptions),
         total_targets = cumsum(targets),
         total_catch_rate = total_receptions / total_targets,
         # Columns to be used later:
         index = 1:n(),
         game_index = paste(&amp;quot;game_&amp;quot;, index, sep = &amp;quot;&amp;quot;),
         game_index = fct_relevel(factor(game_index),
                                  &amp;quot;game_1&amp;quot;, &amp;quot;game_2&amp;quot;, &amp;quot;game_3&amp;quot;,
                                  &amp;quot;game_4&amp;quot;, &amp;quot;game_5&amp;quot;, &amp;quot;game_6&amp;quot;,
                                  &amp;quot;game_7&amp;quot;, &amp;quot;game_8&amp;quot;, &amp;quot;game_9&amp;quot;,
                                  &amp;quot;game_10&amp;quot;, &amp;quot;game_11&amp;quot;, &amp;quot;game_12&amp;quot;,
                                  &amp;quot;game_13&amp;quot;))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Grid approximation provides us with a very simple set of steps to update our
evaluation of JuJu’s performance. The first step is to initialize our grid of
values for JuJu’s &lt;code&gt;p&lt;/code&gt; we’re going to estimate the posterior probability for,
which in this case will be increments of 5% from 0 to 1:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p_grid &amp;lt;- seq(from = 0, to = 1, by = .05)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we define our prior belief for each of the possible values in the grid.
Just to demonstrate, we’ll use a flat prior which means we initially believe
each of the grid values for &lt;code&gt;p&lt;/code&gt; are equally likely. I expand on the prior below,
but for now we use the following:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prior &amp;lt;- rep(1, 21)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we’ll calculate the likelihood for each of the grid values for &lt;code&gt;p&lt;/code&gt; using
the binomial distribution from above. We’ll calculate the likelihood as if only
one game was played (just grabbing the receptions and targets values from the
first row of the data):&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;likelihood &amp;lt;- dbinom(x = juju_games$receptions[1],
                     size = juju_games$targets[1],
                     prob = p_grid)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’re then able to calculate the numerator of Bayes’ theorem by multiplying the
prior by the likelihood, providing the unstandardized posterior for each of the
grid values:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;bayes_numerator &amp;lt;- likelihood * prior&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To arrive at the posterior estimates, we just follow Bayes’ theorem and take
these products and divide them by the sum of the numerators for each value on
the grid. This is of course easy to do with in &lt;code&gt;R&lt;/code&gt; with vectorized operations:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;posterior &amp;lt;- bayes_numerator / sum(bayes_numerator)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our grid approximation for the posterior of JuJu’s catch rate after one game
can easily be viewed:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data.frame(p_grid = p_grid, p_posterior = posterior) %&amp;gt;%
  ggplot(aes(x = p_grid, y = p_posterior)) +
  geom_point(size = 3, color = &amp;quot;darkblue&amp;quot;) + 
  geom_line(color = &amp;quot;darkblue&amp;quot;) +
  # Add a vertical line for JuJu&amp;#39;s observed catch rate:
  geom_vline(xintercept = juju_games$catch_rate[1], color = &amp;quot;darkorange&amp;quot;,
             linetype = &amp;quot;dashed&amp;quot;, size = 3, alpha = .5) +
  # Label!
  labs(x = &amp;quot;Catch rate&amp;quot;, y = &amp;quot;Posterior probability&amp;quot;,
       title = &amp;quot;Posterior approximation for\nJuJu&amp;#39;s catch rate after one game&amp;quot;) +
  # Clean it up:
  theme_bw() + theme(axis.text = element_text(size = 10), 
                     title = element_text(size = 10)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-05-30-bayesian-baby-steps-intro-with-juju_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;384&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This gives us a full posterior distribution for JuJu’s catch rate, rather than
just his observed value of 0.75 after one game (indicated by the orange dashed
line). We can also carry out this grid approximation procedure to generate
posterior distributions for his catch rate after every game based on his
running totals:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data frame by applying the grid approximation steps to each row
# of juju_games:
game_posteriors &amp;lt;- map_dfc(c(1:nrow(juju_games)),
                             function(x) {
                               p_grid &amp;lt;- seq(from = 0, to = 1, by = .05)
                               prior &amp;lt;- rep(1, 21)
                               likelihood &amp;lt;- dbinom(x = juju_games$total_receptions[x],
                                                    size = juju_games$total_targets[x],
                                                    prob = p_grid)
                               bayes_numerator &amp;lt;- likelihood * prior
                               posterior &amp;lt;- bayes_numerator / sum(bayes_numerator)
                               # Return this as a data frame:
                               result &amp;lt;- data.frame(posterior)
                               colnames(result) &amp;lt;- paste(&amp;quot;game_&amp;quot;, x, sep = &amp;quot;&amp;quot;)
                               return(result)
                             })

# Join these columns with p_grid and column for the prior probability:
data.frame(p_grid = p_grid, prior = rep(1 / 21, 21)) %&amp;gt;%
  bind_cols(game_posteriors) %&amp;gt;% 
  # Gather the columns so the data is long, one row for each week and grid value
  gather(key = &amp;quot;game_index&amp;quot;, value = &amp;quot;posterior_prob&amp;quot;, -p_grid) %&amp;gt;%
  # Relevel the game_index variable:
  mutate(game_index = fct_relevel(factor(game_index),
                                  &amp;quot;prior&amp;quot;, &amp;quot;game_1&amp;quot;, &amp;quot;game_2&amp;quot;, &amp;quot;game_3&amp;quot;,
                                  &amp;quot;game_4&amp;quot;, &amp;quot;game_5&amp;quot;, &amp;quot;game_6&amp;quot;,
                                  &amp;quot;game_7&amp;quot;, &amp;quot;game_8&amp;quot;, &amp;quot;game_9&amp;quot;,
                                  &amp;quot;game_10&amp;quot;, &amp;quot;game_11&amp;quot;, &amp;quot;game_12&amp;quot;,
                                  &amp;quot;game_13&amp;quot;)) %&amp;gt;%
  # Visualize the posteriors for each game:
  ggplot(aes(x = p_grid, y = posterior_prob)) + 
  geom_point(size = 2, color = &amp;quot;darkblue&amp;quot;) + 
  geom_line(color = &amp;quot;darkblue&amp;quot;) +
  facet_wrap(~ game_index) +
  # Add vertical lines for each cumulative observed rate
  geom_vline(data = juju_games, 
             aes(xintercept = total_catch_rate), color = &amp;quot;darkorange&amp;quot;,
             linetype = &amp;quot;dashed&amp;quot;, size = 1, alpha = .5) +
  geom_text(data = juju_games, size = 3,
             x = .25, y = .3, aes(label = paste(&amp;quot;Caught&amp;quot;, 
                                                receptions, &amp;quot;of&amp;quot;,
                                                targets, sep = &amp;quot; &amp;quot;))) +
  # Label!
  labs(x = &amp;quot;Catch rate&amp;quot;, y = &amp;quot;Posterior probability&amp;quot;,
       title = &amp;quot;Posterior approximation for JuJu&amp;#39;s catch rate after each game&amp;quot;) +
  # Clean it up:
  theme_bw() + theme(axis.text.y = element_text(size = 10), 
                     axis.text.x = element_text(size = 6),
                     title = element_text(size = 10)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-05-30-bayesian-baby-steps-intro-with-juju_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;This plot gives us a pretty clear view of Bayesian updating. We start with a flat
prior, believing any catch rate between 0 and 1 is equally likely. Then we keep
updating our belief or understanding of what JuJu’s catch is likely to be given
his in game performances. Each dashed line marks his cumulative catch rate which,
should not come as a surprise, is the most likely value after each game. After
his second game where he only caught two of six passes we see a shift in the
distribution. But as we keep observing games, the distributions move and become
more concentrated around his final season catch rate of roughly 0.73. &lt;em&gt;This exact
same process holds and yields the same results if we updated after each individual
target&lt;/em&gt;, after observing one event the resulting posterior becomes the prior for
the next target. Which means that, although I generated the plot above using the
flat prior andcumulative stats after each game, I could’ve also generated the same figure using
the &lt;em&gt;previous game’s posterior as the prior for the following single game performance&lt;/em&gt;.
It’s just simpler from a coding point of view to present all of the data,
but breaking it up individually provides us with the true step-wise view of updating.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;prior-knowledge&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Prior knowledge&lt;/h2&gt;
&lt;p&gt;You’re probably thinking this example is dumb since I started with a flat prior.
And you’re right! Why would anyone believe a receiver’s catch is equally likely
to be 0 as is 1? You might’ve noticed the fact that the posteriors are even just
proportional to the likelihood, so we’re really not taking advantage of the
purpose of a prior distribution. We can use priors to incorporate expert
knowledge into our model, helping us limit parameter values to a range that
makes sense. If you have ever learned about regularized regression, you can
think of priors as accomplishing a similar task. &lt;a href=&#34;http://www2.stat.duke.edu/~rcs46/lectures_2015/14-bayes1/14-bayes3.pdf&#34;&gt;In fact Lasso regression can be intepreted this way.&lt;/a&gt;
People often complain that the choice of prior is subjective, but if you think
data analysis is entirely objective &lt;a href=&#34;https://psyarxiv.com/qkwst/&#34;&gt;then you’re ignorant of the truth.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To choose a reasonable prior for JuJu’s catch rate, we’re going to take a data-driven
approach or use &lt;a href=&#34;https://en.wikipedia.org/wiki/Empirical_Bayes_method&#34;&gt;Empirical Bayes&lt;/a&gt;.
David Robinson has an excellent &lt;a href=&#34;http://varianceexplained.org/r/empirical_bayes_baseball/&#34;&gt;series of posts&lt;/a&gt;
and &lt;a href=&#34;https://www.amazon.com/Introduction-Empirical-Bayes-Examples-Statistics-ebook/dp/B06WP26J8Q&#34;&gt;book&lt;/a&gt;
on Empirical Bayes using baseball batting averages as an example. We’re going to
use the same approach here but in the context of our football problem. We’ll estimate our
prior using the distribution of catch rates by receivers in the 2016 season.
With my &lt;a href=&#34;https://github.com/ryurko/nflWAR&#34;&gt;&lt;code&gt;nflWAR&lt;/code&gt;&lt;/a&gt; package it’s pretty
easy to grab statistics by players for certain positions. Here
we only grab catch rates by WRs in 2016 with at least 25 targets:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# install.packages(&amp;quot;devtools&amp;quot;)
# devtools::install_github(&amp;quot;ryurko/nflWAR&amp;quot;)

library(nflWAR)

# Ignore any messages you see
wr_catch_rates &amp;lt;- get_pbp_data(2016) %&amp;gt;%
  add_positions(2016) %&amp;gt;%
  add_model_variables() %&amp;gt;%
  prepare_model_data() %&amp;gt;%
  add_position_tables() %&amp;gt;%
  join_position_statistics() %&amp;gt;%
  pluck(&amp;quot;WR_table&amp;quot;) %&amp;gt;%
  select(Player_ID_Name, Rec_Perc, Targets) %&amp;gt;%
  filter(Targets &amp;gt;= 25)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next we display our distribution of catch rates in the 2016 season:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wr_catch_rates %&amp;gt;%
  ggplot(aes(x = Rec_Perc)) + 
  geom_density(color = &amp;quot;darkblue&amp;quot;) +
  geom_rug(color = &amp;quot;darkblue&amp;quot;) + 
  theme_bw() +
  labs(x = &amp;quot;Catch rate&amp;quot;, y = &amp;quot;Density&amp;quot;, 
       title = &amp;quot;Distribution of WR catch rates in 2016 (minimum 25 targets)&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-05-30-bayesian-baby-steps-intro-with-juju_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see concentration of catch rates between 0.5 and 0.6 with no receiver
having a catch rate less than 0.4 or above 0.8. Because of the shape of this
distribution and from the fact a receiver’s catch rate must be between 0 and 1,
it makes sense to use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Beta_distribution&#34;&gt;beta distribution&lt;/a&gt;
estimated from this data as our prior. This means we’re assuming&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
p \sim \text{Beta}(\alpha_0, \beta_0)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where we have to estimate the hyper-parameters (parameters for priors), &lt;span class=&#34;math inline&#34;&gt;\(\alpha_0\)&lt;/span&gt;
and &lt;span class=&#34;math inline&#34;&gt;\(\beta_0\)&lt;/span&gt;, from our data. A very simple way to do this in &lt;code&gt;R&lt;/code&gt; is with the
&lt;a href=&#34;https://stats.stackexchange.com/questions/12232/calculating-the-parameters-of-a-beta-distribution-using-the-mean-and-variance&#34;&gt;method of moments using the mean and variance&lt;/a&gt; following the example in Robinson’s post:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Use the fitdistr function from the MASS library:
prior_beta_model &amp;lt;- MASS::fitdistr(wr_catch_rates$Rec_Perc, dbeta, 
                                   start = list(shape1 = 10, shape2 = 10))

# Extract the approximated  priors
alpha0 &amp;lt;- prior_beta_model$estimate[1]
beta0 &amp;lt;- prior_beta_model$estimate[2]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We’ll now follow our grid approximation steps from before but instead use
the approximated beta prior:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a data frame by applying the grid approximation steps to each row
# of juju_games:
new_game_posteriors &amp;lt;- map_dfc(c(1:nrow(juju_games)),
                             function(x) {
                               p_grid &amp;lt;- seq(from = 0, to = 1, by = .05)
                               prior &amp;lt;- dbeta(p_grid, alpha0, beta0)
                               likelihood &amp;lt;- dbinom(x = juju_games$total_receptions[x],
                                                    size = juju_games$total_targets[x],
                                                    prob = p_grid)
                               bayes_numerator &amp;lt;- likelihood * prior
                               posterior &amp;lt;- bayes_numerator / sum(bayes_numerator)
                               # Return this as a data frame:
                               result &amp;lt;- data.frame(posterior)
                               colnames(result) &amp;lt;- paste(&amp;quot;game_&amp;quot;, x, sep = &amp;quot;&amp;quot;)
                               return(result)
                             })

# Join these columns with p_grid and column for the prior probability:
data.frame(p_grid = p_grid, 
           prior = dbeta(p_grid, alpha0, beta0) / sum(dbeta(p_grid, alpha0, beta0))) %&amp;gt;%
  bind_cols(new_game_posteriors) %&amp;gt;% 
  # Gather the columns so the data is long, one row for each week and grid value
  gather(key = &amp;quot;game_index&amp;quot;, value = &amp;quot;posterior_prob&amp;quot;, -p_grid) %&amp;gt;%
  # Relevel the game_index variable:
  mutate(game_index = fct_relevel(factor(game_index),
                                  &amp;quot;prior&amp;quot;, &amp;quot;game_1&amp;quot;, &amp;quot;game_2&amp;quot;, &amp;quot;game_3&amp;quot;,
                                  &amp;quot;game_4&amp;quot;, &amp;quot;game_5&amp;quot;, &amp;quot;game_6&amp;quot;,
                                  &amp;quot;game_7&amp;quot;, &amp;quot;game_8&amp;quot;, &amp;quot;game_9&amp;quot;,
                                  &amp;quot;game_10&amp;quot;, &amp;quot;game_11&amp;quot;, &amp;quot;game_12&amp;quot;,
                                  &amp;quot;game_13&amp;quot;)) %&amp;gt;%
  # Visualize the posteriors for each game:
  ggplot(aes(x = p_grid, y = posterior_prob)) + 
  geom_point(size = 2, color = &amp;quot;darkblue&amp;quot;) + 
  geom_line(color = &amp;quot;darkblue&amp;quot;) +
  facet_wrap(~ game_index) +
  # Add vertical lines for each cumulative observed rate
  geom_vline(data = juju_games, 
             aes(xintercept = total_catch_rate), color = &amp;quot;darkorange&amp;quot;,
             linetype = &amp;quot;dashed&amp;quot;, size = 1, alpha = .5) +
  geom_text(data = juju_games, size = 3,
             x = .25, y = .3, aes(label = paste(&amp;quot;Caught&amp;quot;, 
                                                receptions, &amp;quot;of&amp;quot;,
                                                targets, sep = &amp;quot; &amp;quot;))) +
  # Label!
  labs(x = &amp;quot;Catch rate&amp;quot;, y = &amp;quot;Posterior probability&amp;quot;,
       title = &amp;quot;Posterior approximation for JuJu&amp;#39;s catch rate after each game&amp;quot;) +
  # Clean it up:
  theme_bw() + theme(axis.text.y = element_text(size = 10), 
                     axis.text.x = element_text(size = 6),
                     title = element_text(size = 10)) &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-05-30-bayesian-baby-steps-intro-with-juju_files/figure-html/unnamed-chunk-13-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Now with this informed prior we see a clear difference in the updating procedure
compared to before. With the flat prior the catch rate with the peak posterior
probability was always JuJu’s cumulative catch rate after the game. Now we see
how the prior distribution applies some resistance to the approximated posterior.
For instance, after his second game where he only caught two of six targets,
JuJu’s catch rate dropped to 0.5 but the posterior doesn’t shift as far to the
left because of the prior pulling it back. This process continues the whole way,
with the most likely value after the last game slightly less than JuJu’s catch
rate after the whole season - hence providing some regularization for our
likely value of &lt;code&gt;p&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;All I’ve talked about so far is using grid-based approach to reach this posterior
approximation. But we actually know the analytical calculation&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; for this example’s posterior
since the beta distribution is the conjugate prior to the binomial resulting
in a posterior that also follows the beta distribution (not the
focus of this post but &lt;a href=&#34;https://en.wikipedia.org/wiki/Conjugate_prior#Example&#34;&gt;here’s more info&lt;/a&gt;).
We can calculate this known result quite easily, displaying the density curve
using JuJu’s full season statistics in comparison to the final grid approximation:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Compuate the known posterior for 1000 points from 0 to 1:
known_posterior &amp;lt;- data.frame(p_density = dbeta(seq(0, 1, length.out = 1000),
                             # Add receptions to prior alpha
                             juju_games$total_receptions[nrow(juju_games)] + alpha0,
                             # Add incompletions to prior beta
                             with(juju_games, 
                                  (total_targets[nrow(juju_games)] - 
                                     total_receptions[nrow(juju_games)]) + beta0)),
           p_grid = seq(0, 1, length.out = 1000)) %&amp;gt;%
  ggplot(aes(x = p_grid, y = p_density)) +
  geom_line(color = &amp;quot;darkorange&amp;quot;) +
  # Label!
  labs(x = &amp;quot;Catch rate&amp;quot;, y = &amp;quot;Posterior density&amp;quot;,
       title = &amp;quot;Known posterior distribution using beta prior&amp;quot;) +
  # Clean it up:
  theme_bw() + theme(axis.text = element_text(size = 10), 
                     title = element_text(size = 10)) 


grid_posterior &amp;lt;- new_game_posteriors %&amp;gt;%
  select(game_13) %&amp;gt;%
  bind_cols(data.frame(p_grid = p_grid)) %&amp;gt;%
  ggplot(aes(x = p_grid, y = game_13)) + 
  geom_point(size = 2, color = &amp;quot;darkblue&amp;quot;) + 
  geom_line(color = &amp;quot;darkblue&amp;quot;) +
  # Label!
  labs(x = &amp;quot;Catch rate&amp;quot;, y = &amp;quot;Posterior probability&amp;quot;,
       title = &amp;quot;Grid approximation&amp;quot;) +
  # Clean it up:
  theme_bw() + theme(axis.text = element_text(size = 10), 
                     title = element_text(size = 10)) 

# Install cowplot if you don&amp;#39;t have it!
# install.packages(&amp;quot;cowplot&amp;quot;)
cowplot::plot_grid(known_posterior, grid_posterior)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;http://www.stat.cmu.edu/~ryurko/~ryurko/post/2018-05-30-bayesian-baby-steps-intro-with-juju_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;768&#34; style=&#34;display: block; margin: auto;&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We see that our grid approximation with twenty points does a pretty good job
of capturing the posterior distribution, and if we keep increasing the number
of points in the grid it would follow this posterior exactly. While this is
simple and incredibly easy for this example, when you are fitting a multilevel
model with many parameters this just isn’t practical. Hence the need for more
efficient approaches like quadratic approximation or MCMC. But it’s a great way
to learn how Bayesian updating works.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;next-steps&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Next steps&lt;/h2&gt;
&lt;p&gt;The basic mechanics of Bayesian inference are intuitive and I hope this first
post keeps you interested in learning more. We’re estimating full posterior
distributions for our values of interest, not just point estimates. This means
we’re always acknowledging the uncertainty in our values, something that is
often ignored in sports statistics in particular. Obviously, this catch rate
example is too simplistic - we’re ignoring who is throwing the ball, the
opposing defense, game situation, and also are not distinguishing dropped
balls from bad throws&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;. We’ll keep working with this example as we move towards
a full Bayesian multilevel model to account for what we can (Roger Goodell doesn’t
like to share data). You can take the exact same approach in this example for
any binary outcome: catch/drop, win/lose, success/failure. Someone could easily
take this code above and update their belief regarding a running back’s
Success Rate (% rushes with positive expected points added) as the season
progresses using &lt;a href=&#34;https://github.com/maksimhorowitz/nflscrapR&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next post in this series will either be about sampling from the posterior or
quadratic approximation and Bayesian regression. We’ll get to the fun stuff
like MCMC and multilevel models eventually, but it’s better to build our
understanding first.&lt;/p&gt;
&lt;div class=&#34;figure&#34;&gt;
&lt;img src=&#34;https://media0.giphy.com/media/NAe117ka9jAdi/giphy-downsized.gif?cid=e1bb72ff5b078134556f746f63a8cf95&#34; alt=&#34;Remember, baby steps!&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;Remember, baby steps!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Richard McElreath’s &lt;a href=&#34;https://www.amazon.com/Statistical-Rethinking-Bayesian-Examples-Chapman/dp/1482253445&#34;&gt;Statistical Rethinking&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;David Robinson’s &lt;a href=&#34;http://varianceexplained.org/r/empirical_bayes_baseball/&#34;&gt;blog posts on Empirical Bayes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;My first exposure to multilevel models was reading Jonathan Judge’s excellent work at Baseball Prospectus &lt;a href=&#34;https://www.baseballprospectus.com/news/article/25514/moving-beyond-wowy-a-mixed-approach-to-measuring-catcher-framing/&#34;&gt;about their catcher framing model&lt;/a&gt;.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;“Analytical” is just a fancy way to say it follows from &lt;em&gt;real&lt;/em&gt; math with proofs.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;If only there was some technology to track ball location relative to the receiver and account for this distance in a catch probability model… oh wait, there is…&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>It&#39;s blogging time</title>
      <link>http://www.stat.cmu.edu/~ryurko/post/firstpost/</link>
      <pubDate>Sun, 20 May 2018 00:00:00 +0000</pubDate>
      
      <guid>http://www.stat.cmu.edu/~ryurko/post/firstpost/</guid>
      <description>

&lt;h1 id=&#34;first-post&#34;&gt;First post!&lt;/h1&gt;

&lt;p&gt;Since I have successfully survived my first year of grad school, I&amp;rsquo;ve decided to start blogging while working on my various
projects. I&amp;rsquo;m doing this to: (1) share what I&amp;rsquo;m working on (instead of just tweeting), (2) learn by &lt;em&gt;attempting&lt;/em&gt; to write instructive blog posts, and (3) because multiple people keep telling me this is a good idea&amp;hellip; we&amp;rsquo;ll find out.&lt;/p&gt;

&lt;p&gt;Hopefully you&amp;rsquo;ll find my posts useful and educational, as I&amp;rsquo;ll be providing &lt;code&gt;R&lt;/code&gt; code thanks to the incredible &lt;code&gt;blogdown&lt;/code&gt; package by Yihui Xie (&lt;a href=&#34;https://bookdown.org/yihui/blogdown/&#34; target=&#34;_blank&#34;&gt;more info here&lt;/a&gt;). I&amp;rsquo;ll also be posting here any substantial updates made to the &lt;a href=&#34;https://github.com/maksimhorowitz/nflscrapR&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;nflscrapR&lt;/code&gt;&lt;/a&gt; and &lt;a href=&#34;https://github.com/ryurko/nflWAR&#34; target=&#34;_blank&#34;&gt;&lt;code&gt;nflWAR&lt;/code&gt;&lt;/a&gt; packages (HINT: there will be several this summer).&lt;/p&gt;

&lt;p&gt;Yes, there will be sports posts - but there will be plenty of non-sports posts related to projects I&amp;rsquo;m working on like clustering, multilevel models, copulas, and various programming exercises. &lt;a href=&#34;https://r-nimble.org/&#34; target=&#34;_blank&#34;&gt;NIMBLE anyone?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also I&amp;rsquo;m hoping to keep receiving feedback on what I&amp;rsquo;m working on and any post I write. As the number of posts I write goes up, the probability I write something completely wrong approaches one - so don&amp;rsquo;t hesitate to correct me! Feel free to reach out to me on &lt;a href=&#34;https://twitter.com/Stat_Ron&#34; target=&#34;_blank&#34;&gt;twitter&lt;/a&gt; or email, &lt;a href=&#34;http://www.stat.cmu.edu/~ryurko/&#34; target=&#34;_blank&#34;&gt;contact info here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Stay tuned for more!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
